{
  "articles": [
    {
      "path": "about.html",
      "title": "How Facebook Combats Fake User Accounts",
      "description": "Whether it's fake celebrity investment scams or internet violence led by troll armies, fake accounts not only impact user interests but also harm the commercial interests of the community. This project takes Facebook, the most widely used platform, as an example to explore how it utilizes AI technology to solve these problems and the underlying risks involved.\n",
      "author": [
        {
          "name": "莊惠媤",
          "url": {}
        },
        {
          "name": "董宸賓",
          "url": {}
        },
        {
          "name": "謝長恩 ",
          "url": {}
        }
      ],
      "date": "2024-01-04",
      "contents": "\n\nContents\nThe Harm of Fake Accounts to Social Communities and Users\nExploiting Public Trust in Public Figures for Scams\nErosion of Platform Trust\nNegative Impact on Online Environment\n\nFacebook’s Technological Approach to Combating Fake Accounts\nFacebook’s SybilEdge: A Technological Leap in Detecting Fake Accounts\nUnderstanding SybilEdge\nThe Two-Stage Process of SybilEdge\nThe Efficacy of SybilEdge\nFuture Directions\n\nImage analysis\nImage Recognition Algorithm:\nReverse Image Search:\nSelf-Reporting Mechanism:\n\nConcern\nPrivacy Concerns\nErroneous Blocking\nRestriction and Freedom of Speech\n\nSolutions\nDe-identification Techniques\nExternal Constraints\n\nReference：\n\n\nThe Harm of Fake Accounts to Social Communities and Users\nSocial media platforms, particularly Facebook, have become breeding grounds for fake accounts. These accounts not only undermine user trust but also have far-reaching impacts on the entire social ecosystem.\nfake accountExploiting Public Trust in Public Figures for Scams\nCelebrity Impersonation: Numerous fake accounts use photos and names of celebrities or internet personalities to lure users into joining investment groups, leading to fraudulent activities.\nMisleading Information: These accounts disseminate false information, misleading users and potentially causing financial losses.\nErosion of Platform Trust\nDeclining User Trust: Continuous scam activities have eroded trust in Facebook, resulting in a decrease in its user base.\nAdvertisers Withdrawal: Due to the trust crisis, advertisers have started to reduce their investments on the platform, further affecting its revenue.\nNegative Impact on Online Environment\nErosion of Platform Trust\nDeclining User Trust: Continuous scam activities have eroded trust in Facebook, resulting in a decrease in its user base.\nAdvertisers Withdrawal: Due to the trust crisis, advertisers have started to reduce their investments on the platform, further affecting its revenue.\nNegative Impact on Online Environment\nBot Manipulation: A plethora of bot accounts manipulate discussions and sentiments on the platform, damaging the healthy online communication environment.\nFalse Feedback: Fake accounts provide misleading feedback, impairing the effectiveness of ad targeting and further harming the interests of advertisers and the platform.\nChallenges and Strategies\nFacebook’s Technological Approach to Combating Fake Accounts\nFacebook, as a leading social media platform, has been at the forefront of combating the proliferation of fake accounts. Recognizing the detrimental impact these accounts have on user trust and platform integrity, Facebook has employed a multi-faceted technological strategy.\n\nFacebook’s SybilEdge: A Technological Leap in Detecting Fake Accounts\nFacebook’s ongoing battle against fake accounts has led to the development of SybilEdge, an innovative algorithm designed to scrutinize the patterns of friend requests and responses on the platform. This tool represents a significant stride in Facebook’s efforts to maintain the authenticity of its user base.\nUnderstanding SybilEdge\nSybilEdge operates by analyzing the friend request targets selected by new users and their corresponding acceptance or rejection rates. This approach is grounded in the observation that fake accounts typically exhibit a higher rejection rate compared to genuine accounts. However, the challenge lies in the fact that not all requests from real users are accepted, and not all fake users are consistently rejected.\n\nThe Two-Stage Process of SybilEdge\nTraining the Model: Initially, Facebook observed data over a period, using outputs from behavioral classifiers that flagged accounts as abusive based on actual abuse. This phase helped in setting up the necessary model parameters.\nReal-Time Analysis: For each friend request and response (accept or reject), SybilEdge updates the probability of the requester being fake. This dynamic approach allows for a more nuanced and accurate detection of fake accounts.\nThe Efficacy of SybilEdge\nSybilEdge’s effectiveness is evident in its ability to discern fake accounts with as few as 20 friend requests. This is achieved by leveraging the differences in how users respond to friend requests from real versus fake accounts. The algorithm’s strength lies in its rapid classification, adversarial robustness, low complexity, interpretability, and robust training.\nFuture Directions\nLooking ahead, Facebook aims to further refine the detection of abusive accounts, making decisions faster and more confidently than what SybilEdge currently offers. This involves integrating feature-based and behavior-based models to enhance the platform’s security.\nImage analysis\nIn the digital age, where visual content dominates social media, the authenticity of images has become a critical concern, especially in the context of fake accounts. Image analysis, leveraging advanced technologies like image recognition algorithms and reverse image search, plays a pivotal role in identifying and combating these fraudulent accounts. These technologies not only enhance the security of social media platforms but also preserve the integrity of online interactions.\nImage recognition algorithms use deep learning and AI to scrutinize images for signs of manipulation or forgery. Meanwhile, reverse image search tools help in verifying the originality of profile pictures, detecting whether an image has been copied or reused, which is a common trait of fake accounts. Together, these tools form a formidable line of defense against the misuse of images in creating or sustaining fake online identities.\n\nImage Recognition Algorithm:\nDescription:\nAdvanced image recognition algorithms are pivotal in the fight against fake accounts. Utilizing deep learning and artificial intelligence, these algorithms scrutinize images to identify patterns indicative of forgery or manipulation. They play a crucial role in maintaining the authenticity of user profiles on social media platforms.\nWorking Principle:\nDetection of Anomalies: These algorithms are adept at spotting inconsistencies in images, such as unnatural lighting or distorted facial proportions, which are common in manipulated images.\nPattern Recognition: They analyze the image’s key features, comparing them against known patterns of forgery, to determine the authenticity of the image.\nReverse Image Search:\nDescription:\nReverse image search is a powerful tool used by platforms to verify the originality of profile pictures. It helps in identifying whether an image has been copied from the internet, suggesting potential identity theft or the creation of a fake account.\nWorking Principle:\nImage Comparison: The tool compares the uploaded profile image against a vast database of images available on the internet.\nIdentification of Duplicates: If the same image is found in multiple places, especially on different profiles, it raises a red flag about the genuineness of the account.\nSelf-Reporting Mechanism:\nDescription:\nthis is currently one of the most stable and effective ways to combat misuse. By establishing a reporting system and streamlining the reporting process, aided by AI-assisted review, the public is more willing to report violations.\nWorking Principle:\nPlatforms integrate machine learning and human review, with machine learning automatically sifting through a large volume of reports, while human review teams further scrutinize potential false profiles to enhance detection accuracy.\nConcern\n\nPrivacy Concerns\nDescription:\nThe use of AI in scrutinizing user accounts raises significant privacy concerns. Users are increasingly wary about the extent to which their personal information, including images and interactions, is analyzed by AI systems. This apprehension stems from the fear of intrusive surveillance and misuse of personal data.\nImplications:\nTrust Issues: Users may feel their privacy is being invaded, leading to a lack of trust in the platform.\nData Security: Concerns about how securely the data is stored and who has access to it.\nErroneous Blocking\nDescription:\nAI systems, while sophisticated, are not infallible. There is a risk of legitimate user accounts being incorrectly flagged as fake, leading to unwarranted blocking or restrictions.\nImplications:\nUser Inconvenience: Legitimate users may face the frustration of having their accounts restricted without valid reason.\nAppeal Process: The need for an efficient system to review and rectify such errors.\nRestriction and Freedom of Speech\nDescription:\nThe application of AI in monitoring content can inadvertently lead to excessive censorship. Overzealous filtering could suppress legitimate content, impacting users’ freedom of speech.\nImplications:\nContent Suppression: Legitimate expressions or discussions might be unfairly restricted.\nBalancing Act: Platforms need to find a balance between preventing abuse and preserving freedom of expression.\nSolutions\nDe-identification Techniques\nDe-identification in AI involves methods to protect user privacy by ensuring that personal data cannot be traced back to an individual. This is crucial in maintaining user trust and complying with privacy regulations.\nMulti-Party Computation (MPC): This technique allows multiple parties to jointly compute a function over their inputs while keeping those inputs private. MPC is particularly useful in scenarios where sharing raw data is sensitive or risky.\n\nHomomorphic Encryption: This form of encryption enables computation on ciphertexts, generating an encrypted result that, when decrypted, matches the result of operations performed on the plaintext. It allows for data processing without exposing the actual data.\n\nDifferential Privacy: This approach adds noise to the data or queries on the data to provide privacy guarantees. It ensures that the removal or addition of a single database item does not significantly affect the outcome of any analysis, thus masking the presence of individuals in a dataset.\n\nExternal Constraints\nTo ensure that AI technologies are used responsibly and ethically, external constraints are necessary. These include regulatory frameworks and oversight mechanisms.\nRegulatory Frameworks: Establishing laws and regulations that govern the use of AI, particularly in areas like data privacy, bias prevention, and ethical usage, is crucial. These frameworks provide guidelines and boundaries for AI development and deployment.\nOversight by Third-Party Organizations and Internal Accountability Committees: Independent oversight bodies can monitor AI applications to ensure compliance with ethical standards and regulations. Similarly, internal committees within organizations can oversee AI projects to ensure they align with ethical guidelines and business values.\nReference：\nfb關閉人臉識別技術\n圖像識別算法介紹\n去識別化資料\n\n\n\n",
      "last_modified": "2024-01-04T21:06:54+08:00"
    },
    {
      "path": "ch.html",
      "title": "FB如何打擊虛假帳戶",
      "description": "不管是假名人投資詐騙，抑或水軍帶領的網路暴力，假帳戶不只影響用戶利益，更傷害社群商業利益。本專題以最多人使用的FB為例，探討其如何利用AI技術，解決問題，背後又有何種風險。\n",
      "author": [
        {
          "name": "莊惠媤",
          "url": {}
        },
        {
          "name": "董宸賓",
          "url": {}
        },
        {
          "name": "謝長恩 ",
          "url": {}
        }
      ],
      "date": "2024-01-04",
      "contents": "\n\nContents\n假帳戶對社群與用戶所造成的影響\n利用公眾對名人的信任進行詐騙\n商業威脅\n對線上環境的負面影響\n\nFacebook 技術手段對抗假帳戶\nFacebook 的 SybilEdge：在檢測假帳戶方面的技術飛躍\n了解 SybilEdge\nSybilEdge 的兩階段過程\nSybilEdge 的有效性\n未來方向\n\n圖像分析\n圖像識別算法：\n反向圖像搜索：\n自我舉報機制：\n\n關注點\n隱私擔憂\n錯誤封鎖\n限制與言論自由\n\n解決方案\n去識別化技術\n外部約束\n\nReference：\n\n\n假帳戶對社群與用戶所造成的影響\n在社交媒體的世界中，尤其是在 Facebook，假帳戶的問題日益嚴重。這些帳戶不僅侵蝕了用戶的信任，更對整個社交生態造成了深遠的衝擊。\n假帳戶利用公眾對名人的信任進行詐騙\n名人冒充：許多假帳戶使用名人或網絡紅人的照片和名字來吸引用戶加入投資群組，從而進行詐騙活動。\n誤導信息：這些帳戶散播虛假信息，誤導用戶，可能導致經濟損失。\n商業威脅\n用戶信任下降：持續的詐騙活動已侵蝕了用戶對 Facebook 的信任，導致其用戶基數下降。\n廣告商撤退：由於信任危機，廣告商開始減少在平台上的投資，進一步影響其收入。\n對線上環境的負面影響\n機器人操控：大量機器人帳戶在平台上操縱討論和情緒，損害健康的線上溝通環境。\n虛假反饋：假帳戶提供誤導性反饋，損害廣告定位的有效性，進一步損害廣告商和平台的利益。\nFacebook 技術手段對抗假帳戶\n作為領先的社交媒體平台，Facebook 一直處於對抗假帳戶泛濫的前沿。Facebook 認識到這些帳戶對用戶信任和平台完整性的負面影響，因此採用了多方面的技術策略。\n\nFacebook 的 SybilEdge：在檢測假帳戶方面的技術飛躍\nFacebook 在對抗假帳戶的持續戰鬥中，開發出了 SybilEdge，這是一種創新算法，旨在審查平台上好友請求和回應的模式。這個工具代表了 Facebook 維護用戶基礎真實性方面的重大進步。\n了解 SybilEdge\nSybilEdge 通過分析新用戶選擇的好友請求目標及其對應的接受或拒絕率來運作。這種方法基於這樣的觀察：與真實帳戶相比，假帳戶通常有更高的拒絕率。然而，挑戰在於，並非所有真實用戶的請求都被接受，也不是所有假用戶都被一致拒絕。\n\nSybilEdge 的兩階段過程\n訓練模型：最初，Facebook 觀察了一段時間的數據，使用基於實際濫用行為的行為分類器輸出來標記帳戶。這一階段有助於設置必要的模型參數。\n實時分析：對於每一個好友請求和回應（接受或拒絕），SybilEdge 更新請求者是假帳戶的概率。這種動態方法允許更細緻和準確地檢測假帳戶。\nSybilEdge 的有效性\nSybilEdge 的有效性在於其能夠以僅 20 個好友請求就識別出假帳戶。這是通過利用用戶對真實與假帳戶的好友請求反應差異來實現的。該算法的優勢在於其快速分類、對抗性堅固、低復雜度、可解釋性和堅實的訓練。\n未來方向\n展望未來，Facebook 旨在進一步完善檢測濫用帳戶的方法，使決策比 SybilEdge 目前提供的更快、更有信心。這包括整合基於特徵和基於行為的模型，以增強平台的安全性。\n圖像分析\n在數字時代，視覺內容主導社交媒體，圖像的真實性成為一個關鍵問題，特別是在假帳戶的背景下。利用先進技術，如圖像識別算法和反向圖像搜索，圖像分析在識別和對抗這些欺詐帳戶中扮演著關鍵角色。這些技術不僅增強了社交媒體平台的安全性，還保護了線上互動的完整性。\n圖像識別算法利用深度學習和人工智能審查圖像，以尋找操縱或偽造的跡象。同時，反向圖像搜索工具幫助驗證個人資料圖片的原創性，檢測圖像是否被複製或重複使用，這是假帳戶的常見特徵。這些工具共同構成了一道強大的防線，對抗在創建或維持假線上身份時濫用圖像的行為。\n\n圖像識別算法：\n描述：\n先進的圖像識別算法在對抗假帳戶的戰鬥中起著關鍵作用。這些算法利用深度學習和人工智能審查圖像，以識別表明偽造或操縱的模式。它們在維護社交媒體平台上用戶資料的真實性中扮演著至關重要的角色。\n工作原理：\n異常檢測：這些算法擅長發現圖像中的不一致性，如不自然的光線或扭曲的臉部比例，這些在操縱圖像中很常見。\n模式識別：它們分析圖像的關鍵特徵，並將其與已知的偽造模式進行比較，以確定圖像的真實性。\n反向圖像搜索：\n描述：\n反向圖像搜索是平台用來驗證個人資料圖片原創性的強大工具。它有助於識別圖像是否從互聯網複製，暗示潛在的身份盜用或假帳戶的創建。\n工作原理：\n圖像比較：該工具將上傳的個人資料圖片與互聯網上可用的大量圖像數據庫進行比較。\n複製識別：如果在多個地方發現相同的圖像，特別是在不同的資料上，則會對帳戶的真實性提出質疑。\n自我舉報機制：\n描述：\n這目前是對抗濫用最穩定和有效的方法之一。通過建立舉報系統並簡化舉報流程，輔以 AI 輔助審查，公眾更願意舉報違規行為。\n工作原理：\n平台結合機器學習和人工審查，機器學習自動篩選大量報告，而人工審查團隊進一步審查潛在的假資料，以提高檢測的準確性。\n關注點\n\n隱私擔憂\n描述：\n在審查用戶帳戶時使用 AI 引發了重大的隱私擔憂。用戶對他們的個人信息，包括圖像和互動，被 AI 系統分析的程度越來越警惕。這種憂慮源於對侵入式監視和個人數據濫用的恐懼。\n影響：\n信任問題：用戶可能會感覺他們的隱私被侵犯，導致對平台的不信任。\n數據安全：對數據存儲的安全性以及誰能訪問這些數據的擔憂。\n錯誤封鎖\n描述：\n雖然 AI 系統很先進，但並非萬無一失。合法用戶帳戶被錯誤標記為假帳戶的風險導致無故的封鎖或限制。\n影響：\n用戶不便：合法用戶可能會因帳戶無正當理由被限制而感到沮喪。\n上訴流程：需要一個有效的系統來審查和糾正這些錯誤。\n限制與言論自由\n描述：\n在監控內容中應用 AI 可能無意中導致過度審查。過度積極的過濾可能壓制合法內容，影響用戶的言論自由。\n影響：\n內容壓制：合法的表達或討論可能被不公平地限制。\n平衡行為：平台需要在防止濫用和保護言論自由之間找到平衡。\n解決方案\n去識別化技術\n在 AI 中進行去識別化涉及保護用戶隱私的方法，確保個人數據無法追溯到個人。這在維持用戶信任和遵守隱私法規中至關重要。\n多方計算 (MPC)：這種技術允許多方在保持其輸入私密的同時共同計算一個函數。MPC 在分享原始數據敏感或風險的場景中特別有用。\n\n同態加密：這種加密形式使得在密文上進行計算成為可能，生成的加密結果在解密後與對明文進行的操作結果相匹配。它允許進行數據處理而不暴露實際數據。\n\n差分隱私：這種方法對數據或對數據的查詢添加噪聲，以提供隱私保障。它確保單個數據庫項目的刪除或添加不會顯著影響任何分析的結果，從而掩蓋數據集中個人的存在。\n\n外部約束\n為了確保 AI 技術負責任且道德地使用，必要的外部約束是必需的。這包括監管框架和監督機制。\n監管框架：建立管理 AI 使用的法律和法規，特別是在數據隱私、偏見預防和道德使用等領域，至關重要。這些框架為 AI 的開發和部署提供了指導和界限。\n第三方組織和內部問責委員會的監督：獨立的監督機構可以監控 AI 應用以確保符合道德標準和法規。同樣，組織內部的委員會可以監督 AI 項目，以確保它們與道德指導原則和商業價值保持一致。\nReference：\nfb關閉人臉識別技術\n圖像識別算法介紹\n去識別化資料\n\n\n\n",
      "last_modified": "2024-01-04T21:06:55+08:00"
    },
    {
      "path": "index.html",
      "title": "網站介紹",
      "description": "Welcome to the website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\n\n\nAuthor：董宸賓\nProject：Introduction to CS\n\nSource：\nGithub\n\n\n© 2023 . Some rights reserved. This work is licensed under \nCC BY-NC 4.0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-01-04T21:06:55+08:00"
    }
  ],
  "collections": []
}
